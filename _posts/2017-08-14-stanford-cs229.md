---
layout: post
title: "Stanford CS229"
description: ""
keywords:
category: Machine Learning
tags: []
---


# 第一课

有些理论能够证明在什么情况下机器学习能够做到99%精度之类的支持，
所以还是得学习有些理论基础才行。例如Kaiming就那么厉害。

## 概念

Regression,回归，解决连续问题。

Classification，分类解决Discreetw问题

貌似回归与分类都是监督学习，因为给了答案。

然后是Unsupervized Problem，
既然没给答案，就只能聚类了，（或者我猜用某种方法获得经验？）
聚类在图像处理，例如轮廓线或者音频分离上很有用处。

然后问题参数可能有无限的纬度，如何处理这个问题呢？毕竟计算机不能有无限个，答案是用支持向量机.
SVM，Support Vector Machine。

最后是Reinforce Learning，强化学习，不同于监督与无监督学习，强化学习不是一个On Step Decision，而是一个系列的连续决策与调整。
强化学习的特征是有Reward，通过奖励获取经验，用于机器人，控制helicopter等。

# 第二课

强化学习-连续问题-回归

问题，为什么 artificial algorithm 得到的 hypothesis 假定为线性的，为什么不会是非线性的呢？

答：目前是线性，以后有更复杂的

目标是求 $min \frac{1}{2} \sum_{i=1}^{m}(h_{\theta}(x^{(i)} - y^{(i)})^2 $ ,

有多种方法，例如梯度下降法 (gradient descent)， 但容易到达 Local minimum

推导(链式法则)：
<div>
$$
\begin{align}
\frac{\partial }{\partial \theta_i} J(\theta) & = \frac{\partial }{\partial \theta_i} \frac{1}{2} \left(h_{\theta}(x) - y \right)^2 \\
 & = 2 \cdot \frac{1}{2} (h_{\theta}(x) - y) \cdot \frac{\partial}{\partial \theta_i} (h_\theta (x) - y) \\
 & = (h_{\theta}(x) - y) \cdot \frac{\partial}{\partial \theta_i} (\theta_0 x_0 + \cdots + \theta_n x_n - y)
\end{align}
$$
</div>
